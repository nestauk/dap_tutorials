{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395daaf6",
   "metadata": {},
   "source": [
    "*Hello again!* ðŸ‘‹\n",
    "\n",
    "This notebook is the <u>second</u> part of a **tutorial** on how to  **collect data from Twitter API v2 using Python** ðŸ¤“\n",
    "\n",
    "In this notebook, we make use of the **recent search** endpoint to collect Twitter data on heat pumps and gas boilers from the last 7 days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e44eb4",
   "metadata": {},
   "source": [
    "### Importing packages and loading credentials\n",
    "We start by importing the necessary packages to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d9af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d638597b",
   "metadata": {},
   "source": [
    "We import our *bearer_token* which we previously defined as an environment variable. This way you do not have to expose your credentials in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fbb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = os.environ.get(\"BEARER_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239cd51",
   "metadata": {},
   "source": [
    "### Preparing our API request\n",
    "We will use the recent search endpoint to collect our first set of tweets. To do that we need to define the endpoint URL, the rules clarifying the data we want to collect and other query parameters such as fields to include and maximum number of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_url = \"https://api.twitter.com/2/tweets/search/recent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5cb5cd",
   "metadata": {},
   "source": [
    "We define the following two rules:\n",
    "- tweets matching one of the expressions \"heat pump\"/\"heat pumps\", written in english, which are not retweets;\n",
    "- tweets matching one of the expressions \"gas boiler\"/\"gas boilers\", written in english, which are not retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [\n",
    "    {\"value\": '(\"heat pump\" OR \"heat pumps\") -is:retweet lang:en', \"tag\": \"heat_pump\"},\n",
    "    {\"value\": '(\"gas boiler\" OR \"gas boilers\") -is:retweet lang:en', \"tag\": \"gas_boiler\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d1ad5",
   "metadata": {},
   "source": [
    "We create a dictionary with query parameters, where we pass the following fields:\n",
    "- **tweet.fields**: fields in the tweet object for which we want to collect information, in this example: the tweet unique identifier, the tweet text, the identifier of the user posting the tweet and the date/time the tweet was created;\n",
    "- **user.fields**: fields in the user object for which we want to collect information, in this example: the user unique identifier, name, username, date/time the user created their account, description, user defined location and whether the user is verified or not;\n",
    "- **expansions**: expansion query parameter with info relating to the user. We need to add this in order to receive user data in our response object.\n",
    "- **max_results**: the maximum number of tweets to be retrieved per request to the API, in this case 100 (which is also the maximum allowed).\n",
    "\n",
    "Unlike our previous example, here we do not define the query rules straight away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dceb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_parameters = {\n",
    "    \"tweet.fields\": \"id,text,author_id,created_at\",\n",
    "    \"user.fields\": \"id,name,username,created_at,description,location,verified\",\n",
    "    \"expansions\": \"author_id\",\n",
    "    \"max_results\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb999b47",
   "metadata": {},
   "source": [
    "### Authentication\n",
    "Authentication is done by bearer token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e1d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_headers(bearer_token: str) -> dict:\n",
    "    \"\"\"\n",
    "    Set up the request headers. \n",
    "    Returns a dictionary summarising the bearer token authentication details.\n",
    "\n",
    "    Args:\n",
    "        bearer_token: bearer token credentials\n",
    "    \"\"\"\n",
    "    return {\"Authorization\": \"Bearer {}\".format(bearer_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = request_headers(bearer_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5de79d",
   "metadata": {},
   "source": [
    "### Connecting to endpoint and taking a look at the data\n",
    "We connect to the endpoint and retrieve our first page of data to see what changed in comparison to the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a779cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(endpoint_url: str, headers: dict, parameters: dict) -> json:\n",
    "    \"\"\"\n",
    "    Connects to the endpoint and requests data.\n",
    "    Returns a json with Twitter data if a 200 status code is yielded.\n",
    "    Programme stops if there is a problem with the request and sleeps\n",
    "    if there is a temporary problem accessing the endpoint.\n",
    "\n",
    "    Args:\n",
    "        endpoint_url: url to endpoint we are collecting data from\n",
    "        headers: request headers\n",
    "        parameters: query parameters\n",
    "    \"\"\"\n",
    "    response = requests.request(\n",
    "        \"GET\", url=endpoint_url, headers=headers, params=parameters\n",
    "    )\n",
    "    response_status_code = response.status_code\n",
    "    if response_status_code != 200:\n",
    "        if response_status_code >= 400 and response_status_code < 500:\n",
    "            raise Exception(\n",
    "                \"Cannot get data, the program will stop!\\nHTTP {}: {}\".format(\n",
    "                    response_status_code, response.text\n",
    "                )\n",
    "            )\n",
    "\n",
    "        sleep_seconds = random.randint(5, 60)\n",
    "        print(\n",
    "            \"Cannot get data, your program will sleep for {} seconds...\\nHTTP {}: {}\".format(\n",
    "                sleep_seconds, response_status_code, response.text\n",
    "            )\n",
    "        )\n",
    "        time.sleep(sleep_seconds)\n",
    "        return connect_to_endpoint(endpoint_url, headers, parameters)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643757a3",
   "metadata": {},
   "source": [
    "Let us retrieve the first page of tweets for our first rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c08b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_parameters[\"query\"] = rules[0][\"value\"]\n",
    "json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51816d01",
   "metadata": {},
   "source": [
    "Now the json_response dictionary contains 3 keys: *data*, *includes* and *meta*. The only difference from the previous example is the *includes* field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac49c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57426735",
   "metadata": {},
   "source": [
    "json_response[\"includes\"] is also a dictionary and it contains one key, \"users\", because we are now also collecting user information. If other information such as places/location information was also being collected, then we would have another key in our json_response[\"includes\"] dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb5f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response[\"includes\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f517b8a",
   "metadata": {},
   "source": [
    "This is what each user dictionary looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response[\"includes\"][\"users\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e0ed4",
   "metadata": {},
   "source": [
    "### Collecting tweets from the past 7 days\n",
    "\n",
    "We define a functions to process twitter data and we start the data collection process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_twitter_data(\n",
    "    json_response: json,\n",
    "    query_tag: str,\n",
    "    tweets_data: pd.DataFrame,\n",
    "    users_data: pd.DataFrame,\n",
    ") -> tuple[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Adds new tweet/user information to the table of\n",
    "    tweets/users and saves dataframes as pickle files,\n",
    "    if data is avaiable.\n",
    "\n",
    "    Args:\n",
    "        json_response: new data collected from the endpoint\n",
    "        query_tag: tag/name of the query\n",
    "        tweets_data: tweets info collected so far\n",
    "        users_data: users info collected so far\n",
    "        \n",
    "    Returns:\n",
    "        Returns the tweets and users updated dataframes.\n",
    "    \"\"\"\n",
    "    if \"data\" in json_response.keys():\n",
    "        new = pd.DataFrame(json_response[\"data\"])\n",
    "        tweets_data = pd.concat([tweets_data, new])\n",
    "        tweets_data.reset_index(drop=True, inplace=True)\n",
    "        tweets_data.to_pickle(\"tweets_\" + query_tag + \".pkl\")\n",
    "\n",
    "        if \"users\" in json_response[\"includes\"].keys():\n",
    "            new = pd.DataFrame(json_response[\"includes\"][\"users\"])\n",
    "            users_data = pd.concat([users_data, new])\n",
    "            users_data.drop_duplicates(\"id\", inplace=True)\n",
    "            users_data.reset_index(drop=True, inplace=True)\n",
    "            users_data.to_pickle(\"users_\" + query_tag + \".pkl\")\n",
    "\n",
    "    return tweets_data, users_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856c798",
   "metadata": {},
   "source": [
    "Now that we know what the data looks like, let's start our data collection process!\n",
    "\n",
    "**The data collection process:**\n",
    "- We define empty dataframes where we will store information about tweets and users;\n",
    "- The for loop allows you to go through all your rules;\n",
    "- We update the query parameters query field according to the rule in question;\n",
    "- We connect to the endpoint as in the previous example and process the data, using the process_twitter_data() function;\n",
    "- Then the program sleeps for 5 seconds. This is necessary not to surpass the rate limit. For this specific endpoint and Essential access level, the rate limit is 180 requests/15 minutes per user, which translates into 1 request every 5 seconds so we need to wait for at least 5 seconds before we make another request.\n",
    "- If json_response[\"meta\"] has a next_token (the pagination token) field then it means that we have not reached the final page of tweets, so we add it as a query parameter and collect more tweets;\n",
    "- We repeat the process until  json_response[\"meta\"] no longer contains  next_token field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d11decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(rules)):\n",
    "    tweets_data = pd.DataFrame()\n",
    "    users_data = pd.DataFrame()\n",
    "    \n",
    "    query_parameters[\"query\"] = rules[i][\"value\"]\n",
    "    query_tag = rules[i][\"tag\"]\n",
    "\n",
    "    json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)\n",
    "    tweets_data, users_data = process_twitter_data(\n",
    "        json_response, query_tag, tweets_data, users_data\n",
    "    )\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    while \"next_token\" in json_response[\"meta\"]:\n",
    "        query_parameters[\"next_token\"] = json_response[\"meta\"][\"next_token\"]\n",
    "\n",
    "        json_response = connect_to_endpoint(endpoint_url, headers, query_parameters)\n",
    "        tweets_data, users_data = process_twitter_data(\n",
    "            json_response, query_tag, tweets_data, users_data\n",
    "        )\n",
    "\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d9ff0",
   "metadata": {},
   "source": [
    "### Exercise: Take some time to look at the data we just collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e330dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_hp = pd.read_pickle(\"tweets_heat_pump.pkl\")\n",
    "tweets_gb = pd.read_pickle(\"tweets_gas_boiler.pkl\")\n",
    "\n",
    "users_hp = pd.read_pickle(\"users_heat_pump.pkl\")\n",
    "users_gb = pd.read_pickle(\"users_gas_boiler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b11474",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a96ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545650cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8366f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_gb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collect_twitter_data",
   "language": "python",
   "name": "collect_twitter_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
