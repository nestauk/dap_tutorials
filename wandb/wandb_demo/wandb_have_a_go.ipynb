{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and biases have-a-go\n",
    "\n",
    "We're going to learn how to use weights and biases with the aid of everyone's favourite, the [Kaggle Titanic dataset](https://www.kaggle.com/competitions/titanic/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import wandb\n",
    "\n",
    "import utils\n",
    "from utils import PROJECT_DIR, load_data, SEED\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "WANDB_PROJ = \"Titanic\"  # Change this to your project name!\n",
    "WANDB_USER = os.environ.get(\"wandb_username\")  # Change this to your username!\n",
    "JOB = \"predict survival\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Log a run in Weights and Biases\n",
    "\n",
    "First things first, let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train a classifier to predict whether a passenger survived the Titanic disaster. We'll start simple with a logistic regression model.\n",
    "\n",
    "The first step in using weights and biases is to initialise a **run**. Just as it sounds, a run captures a record of each time you *run* your model. You can choose what metrics, visuals etc you want to record in each run.\n",
    "\n",
    "The way you start a run is like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "        project=WANDB_PROJ,\n",
    "        job_type=JOB,\n",
    "        save_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- I log both `project` and `job_type` because within one project, you might have multiple ML jobs that you want to keep track of. For example, at Nesta, a real use case might be an imputation model to predict and fill in missing values, then another downstream model to make predictions based on the imputed data.\n",
    "- `save_code`: definitely do this! It saves a snapshot of your code at the time of the run. This is mega useful because you might not want to make a Git commit every single time you run your code - you'll do lots of minor changes and tweaking.\n",
    "\n",
    "The run now exists as an object called `run`.\n",
    "\n",
    "If you go to your weights and biases workspace, find the Titanic project and navigate to **Runs**, you should now see that you have a run in progress! Exciting!\n",
    "\n",
    "We will now define and fit a logistic regression model. First of all, we'll define a dict of hyperparameters. The wandb guidance notes that you should keep track of ALL hyperparameters, even if you're just using the defaults. I've been a bit lazy and haven't exactly done that here, but you should!\n",
    "\n",
    "The hyperparameters are stored in a dict because that will make it more convenient when we move on to running sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_config = {'penalty': 'l2',\n",
    "                  'C': 1.0,\n",
    "                  'random_state': SEED,\n",
    "                  'solver':'lbfgs',\n",
    "                  'max_iter':100}\n",
    "    \n",
    "model = LogisticRegression(penalty=log_reg_config['penalty'],\n",
    "                           C=log_reg_config['C'],\n",
    "                           solver=log_reg_config['solver'],\n",
    "                           max_iter=log_reg_config['max_iter'],\n",
    "                           random_state=log_reg_config['random_state'])\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out how the model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "logging.info(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It got 81% accuracy which is not bad for a first go. We will definitely want to store this metric with the run so that we can compare accuracy across different runs. As a bonus, we'll also create and log a confusion matrix.\n",
    "\n",
    "The way you log things to the run is extremely simple for single metrics, and not much more complicated for artefacts like tables and graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.summary[\"accuracy\"] = accuracy\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "cm = pd.DataFrame(cm)\n",
    "logging.info(f\"Confusion matrix:\\n{cm}\")\n",
    "\n",
    "# Log confusion matrix\n",
    "wb_confusion_matrix = wandb.Table(data=cm, columns=[\"0\", \"1\"])\n",
    "run.log({\"confusion_matrix\": wb_confusion_matrix})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're done, so we'll finish the run! You can go to the **Runs** page and inspect :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sweep over hyperparameters\n",
    "\n",
    "Ok, we have a model. It got only 81% accuracy, so we might want to try some different hyperparameters to see if we can improve it. Weights and biases sweeps are a really convenient way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus:** within a sweep you can not just try different hyperparameters, but you can also try out different models or data preprocessing steps, embeddings models etc. You can see an example of how to do this in the script `sweep_different_classifiers.py`. There is guidance on how to nest parameters in your sweep config [here](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
