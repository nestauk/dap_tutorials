{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights and biases have-a-go\n",
    "\n",
    "We're going to learn how to use weights and biases with the aid of everyone's favourite, the [Kaggle Titanic dataset](https://www.kaggle.com/competitions/titanic/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import wandb\n",
    "\n",
    "from utils import PROJECT_DIR, load_data, SEED\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "WANDB_PROJ = \"Titanic\"  # Change this to your project name!\n",
    "WANDB_USER = os.environ.get(\"wandb_username\")  # Change this to your username!\n",
    "JOB = \"predict survival\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Log a run in Weights and Biases\n",
    "\n",
    "First things first, let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train a classifier to predict whether a passenger survived the Titanic disaster. We'll start simple with a logistic regression model.\n",
    "\n",
    "The first step in using weights and biases is to initialise a **run**. Just as it sounds, a run captures a record of each time you *run* your model. You can choose what metrics, visuals etc you want to record in each run.\n",
    "\n",
    "The way you start a run is like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "        project=WANDB_PROJ,\n",
    "        job_type=JOB,\n",
    "        save_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- I log both `project` and `job_type` because within one project, you might have multiple ML jobs that you want to keep track of. For example, at Nesta, a real use case might be an imputation model to predict and fill in missing values, then another downstream model to make predictions based on the imputed data.\n",
    "- `save_code`: definitely do this! It saves a snapshot of your code at the time of the run. This is mega useful because you might not want to make a Git commit every single time you run your code - you'll do lots of minor changes and tweaking.\n",
    "\n",
    "The run now exists as an object called `run`.\n",
    "\n",
    "If you go to your weights and biases workspace, find the Titanic project and navigate to **Runs**, you should now see that you have a run in progress! Exciting!\n",
    "\n",
    "We will now define and fit a logistic regression model. First of all, we'll define a dict of hyperparameters. The wandb guidance notes that you should keep track of ALL hyperparameters, even if you're just using the defaults. I've been a bit lazy and haven't exactly done that here, but you should!\n",
    "\n",
    "The hyperparameters are stored in a dict because that will make it more convenient when we move on to running sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_config = {'penalty': 'l2',\n",
    "                  'C': 1.0,\n",
    "                  'random_state': SEED,\n",
    "                  'solver':'lbfgs',\n",
    "                  'max_iter':100}\n",
    "    \n",
    "model = LogisticRegression(penalty=log_reg_config['penalty'],\n",
    "                           C=log_reg_config['C'],\n",
    "                           solver=log_reg_config['solver'],\n",
    "                           max_iter=log_reg_config['max_iter'],\n",
    "                           random_state=log_reg_config['random_state'])\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out how the model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "logging.info(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It got 81% accuracy which is not bad for a first go. We will definitely want to store this metric with the run so that we can compare accuracy across different runs. As a bonus, we'll also create and log a confusion matrix.\n",
    "\n",
    "The way you log things to the run is extremely simple for single metrics, and not much more complicated for artefacts like tables and graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.summary[\"accuracy\"] = accuracy\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "cm = pd.DataFrame(cm)\n",
    "logging.info(f\"Confusion matrix:\\n{cm}\")\n",
    "\n",
    "# Log confusion matrix\n",
    "wb_confusion_matrix = wandb.Table(data=cm, columns=[\"0\", \"1\"])\n",
    "run.log({\"confusion_matrix\": wb_confusion_matrix})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're done, so we'll finish the run! You can go to the **Runs** page and inspect :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sweep over hyperparameters\n",
    "\n",
    "Ok, we have a model. It got only 81% accuracy, so we might want to try some different hyperparameters to see if we can improve it. Weights and biases [sweeps](https://docs.wandb.ai/guides/sweeps) are a really convenient way to do this.\n",
    "\n",
    "To run a sweep, we need:\n",
    "- a config that contains: `method` ie the search method; `metric` ie the metric to optimise for; `parameters` ie values of the different hyperparameters to try\n",
    "- a functions that define the training procedure\n",
    "- a sweep agent that runs the sweep\n",
    "\n",
    "You can search the hyperparameter space using either Bayesian search, grid search or random search. Random search is quick and efficient and is the approach we'll take here. For a random search, you need to tell weights and biases how many random possible combinations of hyperparameters to try.\n",
    "\n",
    "The code below sets the number of runs and the sweep config, and repackages the code we used for model training earlier as a `train()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUNS = 5 # How many different hyperparameter combinations to try aka how many different runs\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'random',  # grid, random, or bayes. Random picks random combinations of hyperparameter values. Grid picks all possible combinations. Bayes picks based on past results.\n",
    "    'metric': {\n",
    "      'name': 'accuracy',\n",
    "      'goal': 'maximize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'penalty': {'values': ['l2', None]},\n",
    "        'solver': {'values': ['lbfgs']}, # even if there are hyperparameters that you aren't planning on varying, it is still good to record these!\n",
    "        'C': {\n",
    "            'values': [0.01, 0.1, 1]\n",
    "        },\n",
    "        'max_iter': {\n",
    "            'values': [10, 100, 1000]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def train(config, X_train, X_test, y_train, y_test):\n",
    "    # Build the model\n",
    "    model = LogisticRegression(C=config['C'],\n",
    "                               max_iter=config['max_iter'],\n",
    "                               penalty = config['penalty'],\n",
    "                               random_state=SEED,\n",
    "                               solver = config['solver'])\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    preds = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    cm = pd.DataFrame(cm)\n",
    "    \n",
    "    return accuracy, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function `main()` that contains everything we want to happen within a single run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    wandb.init(project=WANDB_PROJ,\n",
    "               job_type=JOB,\n",
    "               save_code=True,\n",
    "               tags=[\"logistic regression\"])\n",
    "    X_train, X_test, y_train, y_test = load_data()\n",
    "    accuracy, cm = train(wandb.config,  X_train, X_test, y_train, y_test)\n",
    "    wandb.log({\"accuracy\": accuracy})\n",
    "    \n",
    "    # Log confusion matrix\n",
    "    wb_confusion_matrix = wandb.Table(data=cm, columns=[\"0\", \"1\"])\n",
    "    wandb.log({\"confusion_matrix\": wb_confusion_matrix})\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to kick off a sweep!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=WANDB_PROJ)\n",
    "\n",
    "wandb.agent(sweep_id, entity=WANDB_USER, function=main,\n",
    "                count=N_RUNS\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now navigate back to your weights and biases Titanic project and navigate to **Sweeps**. You should see the details of the sweep we just ran. The sweep info will tell you some useful things like:\n",
    "- Which run within the sweep got the best accuracy\n",
    "- Parameter importance: which hyperparameters contributed most to the model's performance. This might inform you whether to try more runs at smaller increments of those hyperparameters.\n",
    "\n",
    "**Bonus:** within a sweep you can not just try different hyperparameters, but you can also try out different models or data preprocessing steps, embeddings models etc. You can see an example of how to do this in the script `sweep_different_classifiers.py`. There is guidance on how to nest parameters in your sweep config [here](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration).\n",
    "\n",
    "That's all for now :) See more detailed info and a handy colab notebook on sweeps [here](https://docs.wandb.ai/guides/sweeps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wandb_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
